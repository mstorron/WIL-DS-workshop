{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32225ef8-fe7b-490e-81f2-cfa8530789f9",
   "metadata": {},
   "source": [
    "## French formality detector in translation memories\n",
    "\n",
    "Imagine you've been working with a client for 3 years, building up a large TM in English-French. Suddenly, the client decides they want to change their tone - they want to switch from formal French (using 'vous') to informal French (using 'tu') in all their content. You have a huge TM with many thousands of segments.\n",
    "This would be a huge task to do manually - you'd need to check every segment.\n",
    "\n",
    "### What this notebook does\n",
    "- Takes a TMX file containing English-French translations\n",
    "- Identifies segments using formal French (\"vous\")\n",
    "- Creates a list of segments that need to be changed to informal French (\"tu\")\n",
    "- Exports results to a CSV file\n",
    "\n",
    "### Recommended: familiarity with\n",
    "* Python basics (variables, functions)\n",
    "* Environment management\n",
    "* Directory structure\n",
    "* Package management\n",
    "* Git for version control\n",
    "* TMX/XML structure\n",
    "* French formality rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a024b9-ad69-49b7-9648-40a71ddeb1dc",
   "metadata": {},
   "source": [
    "## 1. Setting up our tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ad5fe8-7f2f-4b86-8456-f28f12e5a791",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load packages\n",
    "import os # Operating System interface for cross-platform file and directory operations\n",
    "import xml.etree.ElementTree as ET # built-in xml parser for handling TMX files\n",
    "import pandas as pd #data manipulation library for creating and managing our translation dataframes\n",
    "import spacy # NLP library\n",
    "from collections import Counter #to count occurrences of items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1ad394-3c24-4f98-ac0b-8a2781ae63c8",
   "metadata": {},
   "source": [
    "## 2. Spacy demo \n",
    "Before we analyze our TMX, let's see how we can detect language patterns. We'll use spaCy, a language analysis tool.\n",
    "\n",
    "### 2.1 POS and morphological tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1980d43-25db-45df-a5b1-90038662de2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English language model from spaCy (this is like loading a tool that helps us understand language)\n",
    "# spacy uses the universal dependencies for most languages https://github.com/UniversalDependencies\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Our example sentence we want to analyze\n",
    "text = \"I love New York\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc: # For each word, we'll get 4 pieces of information:\n",
    "   token_text = token.text  # 1. token_text: the actual word\n",
    "   token_pos = token.pos_   # 2. token_pos: Part of speech \n",
    "   token_dep = token.dep_    # 3. token_dep: dependencies\n",
    "   token_morph = token.morph  # 4. token_morph: morphological information\n",
    "   print(f\"{token_text:<10}{token_pos:<10}{token_dep:<10}{token_morph}\") # Print info in a neat format :<10 means \"use 10 spaces\" for nice alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686b6057-a944-4048-9106-dedd2e2c499e",
   "metadata": {},
   "source": [
    "### 2.1 Dependency and entity information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f875ab-8952-43a8-aba7-658a48bd14ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"dep\") #display dependency information\n",
    "displacy.render(doc, style=\"ent\") #display entity information "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e96285a-3319-4449-80b8-068f80ec971c",
   "metadata": {},
   "source": [
    "## 3. Working with Translation Memories\n",
    "Now let's work with our TM\n",
    "\n",
    "### 3.1 Defining a function to parse TMX Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779dce35-eb62-4b47-9016-e15a9b40b3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tmx_to_df(tmx_file_path):\n",
    "    \"\"\"\n",
    "    Parse a TMX file and extract source (English), target (French) texts and their UIDs\n",
    "    into a pandas dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    tmx_file_path (str): Path to your TMX file\n",
    "    \n",
    "    Returns:\n",
    "    pandas.dataframe: dataframe with columns ['tuid', 'source', 'target']\n",
    "    \n",
    "    Example usage:\n",
    "        df = parse_tmx_to_df(file_path)\n",
    "    \"\"\"\n",
    "    # Define the namespace mapping\n",
    "    namespaces = {'xml': 'http://www.w3.org/XML/1998/namespace'}\n",
    "    \n",
    "    # Parse the xml file\n",
    "    tree = ET.parse(tmx_file_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # Prepare lists to store the data\n",
    "    data = []\n",
    "    \n",
    "    # Find all translation units\n",
    "    for tu in root.findall('.//tu'):\n",
    "        # Get the tuid from the tuid attribute\n",
    "        tuid = tu.get('tuid')\n",
    "            \n",
    "        # Find the English and French segments using namespace\n",
    "        en_tuv = tu.find(\"./tuv[@{%s}lang='en']\" % namespaces['xml'])\n",
    "        fr_tuv = tu.find(\"./tuv[@{%s}lang='fr']\" % namespaces['xml'])\n",
    "        \n",
    "        en_seg = en_tuv.find('seg')\n",
    "        fr_seg = fr_tuv.find('seg')\n",
    "        \n",
    "        source = en_seg.text\n",
    "        target = fr_seg.text\n",
    "        \n",
    "        # Add to data list\n",
    "        data.append({\n",
    "            'tuid': tuid,\n",
    "            'source': source,\n",
    "            'target': target\n",
    "        })\n",
    "    \n",
    "    # Create dataframe and sort by tuid\n",
    "    df = pd.DataFrame(data)\n",
    "    df['tuid'] = df['tuid'].astype(int)\n",
    "    df = df.sort_values('tuid')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667929d9-c59b-47d1-b71d-c6439509e647",
   "metadata": {},
   "source": [
    "### 3.2 Using our parsing function to parse our TM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af0bfd8-2645-44c3-81a1-ba72bda87580",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Define input and output directories\n",
    "    input_dir = 'input_data'\n",
    "    output_dir = 'output_data'\n",
    "    \n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(input_dir, exist_ok=True)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Construct file paths using os.path.join so it can be run on any machine\n",
    "    input_file = os.path.join(input_dir, 'alice.tmx')\n",
    "    output_file = os.path.join(output_dir, 'translations.csv')\n",
    "    \n",
    "    # Load TMX and create dataframe\n",
    "    df = parse_tmx_to_df(input_file)\n",
    "    \n",
    "    # Save to csv\n",
    "    df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "\n",
    "    # Display TMX length\n",
    "    print(\"\\nTMX length:\")\n",
    "    print(f\"Total number of segments: {len(df)}\")\n",
    "    \n",
    "    # Display settings and preview\n",
    "    pd.set_option('display.max_colwidth', 150) \n",
    "    display(df.head()) #display first 5 rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e46cfd0-c464-4af8-9570-769e4b4e7c49",
   "metadata": {},
   "source": [
    "## 4. Detecting formal language\n",
    "Now that we can load TMX files, let's analyze the French text for formality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12554959-8dcc-4a1d-ba76-b6aa4466b35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the French model\n",
    "nlp = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dee40ea-bce6-4693-b87b-34c6005aa9bd",
   "metadata": {},
   "source": [
    "### 4.1 Defining a function to detect formality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b38cf22-c4e0-47bf-b3e2-e2de934ed2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_formality_type(text, nlp):\n",
    "    \"\"\"\n",
    "    Check formality type in French text based on number and person.\n",
    "    \n",
    "    Parameters:\n",
    "    text (str): French text to analyze\n",
    "    nlp: spaCy language object\n",
    "    \n",
    "    Returns:\n",
    "    str: 'Formal' if all second-person forms are plural\n",
    "         'Informal' if all second-person forms are singular\n",
    "         'Not Applicable' if both types are found or neither is found\n",
    "    \"\"\"\n",
    "\n",
    "    plural_found = False\n",
    "    singular_found = False\n",
    "\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    for token in doc:\n",
    "        text_lower = token.text.lower()\n",
    "\n",
    "        # Check for specific pronouns and their forms\n",
    "        #being extra careful here and adding the specific words\n",
    "        #not all surface forms have the expected metadata, so we risk missclassification\n",
    "        if text_lower in [\"vous\", \"vôtre\", \"vôtres\", \"votre\", \"vos\"]:\n",
    "            plural_found = True\n",
    "        elif text_lower in [\"tu\", \"te\", \"t'\", \"toi\", \"tien\", \"tienne\", \"tiens\", \"tiennes\", \"ton\", \"ta\", \"tes\"]:\n",
    "            singular_found = True\n",
    "        \n",
    "        # Check for verbs and auxiliaries for second-person forms\n",
    "        if token.pos_ in [\"VERB\", \"AUX\"]:\n",
    "            person = token.morph.get(\"Person\")\n",
    "            number = token.morph.get(\"Number\")\n",
    "            if person == [\"2\"]:\n",
    "                if number == [\"Plur\"]:\n",
    "                    plural_found = True\n",
    "                elif number == [\"Sing\"]:\n",
    "                    singular_found = True\n",
    "\n",
    "    # Determine return value based on found forms\n",
    "    if plural_found and singular_found:\n",
    "        return 'Not Applicable'  # Both found\n",
    "    elif plural_found:\n",
    "        return 'Formal'  # All forms are plural\n",
    "    elif singular_found:\n",
    "        return 'Informal'  # All forms are singular\n",
    "    else:\n",
    "        return 'Not Applicable'  # Neither found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2129fd2a-89f1-4e2a-bebe-ede12210231f",
   "metadata": {},
   "source": [
    "## 5. Using our formality function with our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c3c03d-be06-437d-88cc-da543d9bed67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "os.makedirs('output_data', exist_ok=True)\n",
    "\n",
    "# Add formality type column to DataFrame\n",
    "df['formality_type'] = df['target'].apply(lambda x: check_formality_type(x, nlp))\n",
    "\n",
    "\n",
    "# Save results\n",
    "output_file = os.path.join('output_data', 'translations_with_formality_types.csv')\n",
    "df.to_csv(output_file, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adbe005-662f-4ea9-bf7e-1dd53c8985cd",
   "metadata": {},
   "source": [
    "### 5.1 Show a sample of our data with formality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de3de30-92f2-4477-9bfb-d818089ea427",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample of all segments:\")\n",
    "display(df[['tuid', 'source', 'target', 'formality_type']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068aacf6-12a5-4074-a8a9-69fd18446d7f",
   "metadata": {},
   "source": [
    "### 5.2 Show a sample of our data with formality, only formal segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4f15e3-5cf7-4cd4-a330-4648eeb523ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFormal segments (plural):\")\n",
    "display(df[df['formality_type'] == 'Formal'][['tuid', 'source', 'target', 'formality_type']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb404302-db3d-466f-91c1-2e747d6e8833",
   "metadata": {},
   "source": [
    "## 6. Statistics and distribution of formalities in our TM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef73356-351e-44f8-a83e-34982e3bc415",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDistribution of formality types:\")\n",
    "print(df['formality_type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb1741e-591c-44ea-885f-45b58ac0919d",
   "metadata": {},
   "source": [
    "## 7. Testing and verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb2630a-c776-4a1f-ae75-86c4a0165ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Ceci ton exemple et votre document.\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text:<10}{token.pos_:<10}{token.dep_:<10}{token.morph}\")\n",
    "\n",
    "#test our function with new content\n",
    "check_formality_type(doc, nlp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
